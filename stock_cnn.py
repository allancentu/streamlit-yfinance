# -*- coding: utf-8 -*-
"""Stock CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16K2o73I4vzjnK-EOv_8cQo8_c-uQ94Ai

# **Utilizando CNNs para análise gráfica de ações**
O objetivo desse exercício é avaliar se CNNs são capazes de aprender comportamentos ligados a [análise gráfica](https://www.modalmais.com.br/blog/analise-grafica/) (também chamada de análise técnica), utilizados por economistas e profissionais do mercado financeiro para prever variações de curto-prazo em preços de ativos a partir de determinados padrões visuais examinados em gráficos que correlacionam volume e preços negociados.

A análise gráfica é diferente da [análise fundamentalista](https://warren.com.br/magazine/analise-fundamentalista-e-tecnica/), focada em analisar fundamentos financeiros da empresa (crescimento de receita, composição do board, resultados operacionais, etc.) para definir estratégias de investimento de médio e longo-prazo.

Seu principal objetivo é especular com preços no curto-prazo para ganhar com a volatilidade dos mercados. Por isso, é muito utilizada em negociações de day trade ou de ativos com alta volatilidade (ações, crypto, índices, etc.).
"""

!pip install -q yfinance mplfinance tensorflow scikit-learn Pillow TA-Lib pytickersymbols

# Bibliotecas básicas
import pandas as pd
import numpy as np
import random as rd
import gc
import shutil
import zipfile
from pathlib import Path
from google.colab import drive

# Bibliotecas financeiras e visuais
import yfinance as yf
import talib
import matplotlib.pyplot as plt
import mplfinance as mpf
from PIL import Image
from pytickersymbols import PyTickerSymbols

# Tensorflow e Scikit-Learn
import tensorflow as tf
import keras
from keras import saving
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.metrics import confusion_matrix, f1_score, ConfusionMatrixDisplay, classification_report, accuracy_score, hamming_loss

"""# Utils para gerar datasets
Uma vez que a análise técnica é baseada na observação de comportamentos visuais em gráficos, usar CNNs se torna uma opção se elas forem capazes de aprender [os mesmos comportamentos visuais observados por economistas](https://www.warriortrading.com/candlestick-charts/) nessa modalidade.

Para isso, é necessário gerar um dataset com um grande volume de dados de ativos financeiros representados visualmente, utilizando o padrão de [candlestick](https://www.jaspersoft.com/articles/what-is-a-candlestick-chart) comum nesse tipo de análise.

Na célula abaixo, foram construídas quatro funções que realizam esse processo e transformam o resultado em um dataset treinável pelo Keras.
"""

# ============================================================================
# Constantes de nível de módulo e cache
# ============================================================================

# Cache dos nomes das funções de padrão TA-Lib no nível do módulo para desempenho
# Isso evita chamar dir(talib) milhares de vezes durante a geração de amostras
_TALIB_PATTERN_FUNCTIONS = [f for f in dir(talib) if f.startswith('CDL')]


# ============================================================================
# Funções da API Pública
# ============================================================================

def download_data(tickers, period='1mo', interval='1d'):
    """
    Baixa dados históricos de ações usando yfinance.

    Args:
        tickers (list ou str): Símbolos dos tickers (ex: ['AAPL', 'MSFT'] ou 'AAPL').
        period (str): Período dos dados (ex: '1mo', '1d', '1y'). Padrão: '1mo'.
        interval (str): Intervalo entre dados (ex: '1m', '5m', '1h', '1d'). Padrão: '1m'.

    Returns:
        pd.DataFrame: Dados OHLCV com colunas em minúsculo e coluna 'ticker'.
                     DataFrame vazio se não houver dados disponíveis.
    """
    print(f"Baixando dados: {tickers}, período={period}, intervalo={interval}")

    # Download em lote é mais eficiente que múltiplas requisições individuais
    downloaded_data = yf.download(tickers, period=period, interval=interval, auto_adjust=True,
                                  group_by='ticker', threads=True, progress=False)

    if downloaded_data.empty:
        print("Aviso: Nenhum dado disponível para download.")
        return pd.DataFrame()

    # Ticker único: estrutura de colunas é diferente (sem MultiIndex)
    if len(tickers) == 1:
        # Achata colunas MultiIndex se presentes
        if isinstance(downloaded_data.columns, pd.MultiIndex):
            downloaded_data.columns = downloaded_data.columns.get_level_values(0)
        downloaded_data.columns = downloaded_data.columns.str.lower()
        downloaded_data['ticker'] = tickers[0]
        print(f"Download concluído: {len(downloaded_data)} registros para {tickers[0]}")
        return downloaded_data

    # Múltiplos tickers: concatena dados de cada ticker individualmente
    ticker_dataframes = []
    for ticker_symbol in tickers:
        if ticker_symbol not in downloaded_data.columns.get_level_values(0):
            print(f"Aviso: {ticker_symbol} não encontrado nos dados baixados.")
            continue

        ticker_df = downloaded_data[ticker_symbol].copy()

        # Verifica se o dataframe está vazio ou se contém apenas NaNs (falha no download)
        if ticker_df.empty or ticker_df.isna().all().all():
            print(f"Aviso: {ticker_symbol} sem dados válidos (possivelmente delisted ou erro no download).")
            continue

        # Achata colunas MultiIndex se presentes
        if isinstance(ticker_df.columns, pd.MultiIndex):
            ticker_df.columns = ticker_df.columns.get_level_values(0)
        ticker_df.columns = ticker_df.columns.str.lower()
        ticker_df['ticker'] = ticker_symbol
        ticker_dataframes.append(ticker_df)

    result = pd.concat(ticker_dataframes, ignore_index=False) if ticker_dataframes else pd.DataFrame()
    print(f"Download concluído: {len(result)} registros totais para {len(ticker_dataframes)} tickers")
    return result

def get_random_tickers(num_tickers=100):
    """
    Obtém uma lista aleatória de tickers de índices dos EUA (S&P 500, NASDAQ 100, DOW JONES, S&P 100).

    Args:
        num_tickers (int): Quantidade de tickers desejada.

    Returns:
        list: Lista de símbolos de tickers (str) únicos.
    """
    print(f"\nObtendo {num_tickers} tickers aleatórios dos principais índices dos EUA...")

    stock_data = PyTickerSymbols()

    # Índices baseados nos EUA disponíveis na biblioteca
    us_indexes = ['DOW JONES', 'NASDAQ 100', 'S&P 100', 'S&P 500']

    all_stocks = []
    for index_name in us_indexes:
        try:
            stocks = stock_data.get_stocks_by_index(index_name)
            all_stocks.extend(stocks)
        except Exception as e:
            print(f"Aviso: Erro ao obter ações do índice {index_name}: {e}")

    # Extrai símbolos e remove duplicatas
    unique_tickers = set()
    for stock in all_stocks:
        # pytickersymbols retorna uma lista de dicionários, o símbolo está em 'symbol'
        symbol = stock['symbol']
        unique_tickers.add(symbol)

    unique_tickers_list = list(unique_tickers)

    # Seleciona aleatoriamente a quantidade desejada
    if len(unique_tickers_list) > num_tickers:
        selected_tickers = rd.sample(unique_tickers_list, num_tickers)
    else:
        print(f"Aviso: Quantidade solicitada ({num_tickers}) maior que o total disponível ({len(unique_tickers_list)}). Retornando todos.")
        selected_tickers = unique_tickers_list

    # Ordena para apresentação consistente
    selected_tickers.sort()

    print(f"Sucesso: {len(selected_tickers)} tickers selecionados.")
    print(f"Tickers: {', '.join(selected_tickers)}")

    return selected_tickers

def generate_samples(stock_data, tickers=None, num_samples=100, output_directory='/content/samples',
                     batch_size=10, top_k_cdl_features=20, export_to_drive=False, export_filename='dataset'):
    """
    Gera imagens de candlestick para treinamento de modelos de ML.

    Cada imagem contém 30 candles e é rotulada com TODOS os padrões TA-Lib detectados
    e direções de preço futuras (next1, next5, next30).
    Para cada amostra, gera cinco variações com estilos visuais diferentes.

    Args:
        stock_data (pd.DataFrame): Dados com colunas 'ticker', 'open', 'high', 'low', 'close', 'volume'.
        tickers (list, optional): Tickers para gerar amostras. Se None, usa todos os tickers disponíveis.
        num_samples (int): Quantidade de amostras por ticker.
        output_directory (str): Diretório de saída das imagens. Padrão: '/content/samples'.
        batch_size (int): Número de amostras processadas por lote (padrão: 10).
        top_k_cdl_features (int): Número de melhores features CDL a manter (padrão: 20).
        export_to_drive (bool): Se True, exporta dataset para Google Drive como zip. Padrão: False.
        export_filename (str): Nome do arquivo zip para exportação (sem extensão). Padrão: 'dataset'.

    Returns:
        pd.DataFrame: Metadados com colunas 'filename', detecções de padrões (1/0) e direções de preço.
    """
    # Tamanho fixo da amostra
    SAMPLE_SIZE = 30

    # Obtém configuração
    chart_styles = _get_chart_styles()
    columns_to_normalize, column_rename_mapping = _get_normalization_config()

    # Infere tickers se não fornecidos
    if tickers is None:
        tickers = stock_data['ticker'].unique().tolist()

    print(f"\nGerando imagens: {num_samples} amostras/ticker (5 variações cada), {SAMPLE_SIZE} períodos/amostra")
    print(f"Calculando padrões TA-Lib e direções de preço (next1, next5, next30)")

    # Configura diretório de saída
    image_metadata = []
    output_path = Path(output_directory)
    output_path.mkdir(parents=True, exist_ok=True)

    # Processa cada ticker
    for ticker_symbol in tickers:
        ticker_metadata = process_ticker_samples(
            ticker_symbol, stock_data, num_samples, batch_size, chart_styles, output_path, columns_to_normalize, column_rename_mapping
        )
        image_metadata.extend(ticker_metadata)

    print(f"\nGeração concluída: {len(image_metadata)} imagens totais. Exemplos aleatórios abaixo:")

    # Mostra amostras aleatórias (garantindo que não sejam variações da mesma amostra)
    if len(image_metadata) >= 4:
        # Agrupa índices por identificador de amostra (ticker + número da amostra)
        # Formato do arquivo: {ticker}_sample_{num}_{var}.png
        sample_groups = {}
        for idx, meta in enumerate(image_metadata):
            filename = Path(meta['filename']).name
            # Extrai o ID da amostra removendo a variação e extensão
            # Ex: TLK_sample_1_1.png -> TLK_sample_1
            sample_id = "_".join(filename.split('_')[:-1])

            if sample_id not in sample_groups:
                sample_groups[sample_id] = []
            sample_groups[sample_id].append(idx)

        # Seleciona 4 grupos de amostras únicos
        unique_sample_ids = list(sample_groups.keys())
        if len(unique_sample_ids) >= 4:
            selected_ids = rd.sample(unique_sample_ids, 4)

            # Para cada amostra selecionada, escolhe uma variação aleatória
            sample_indices = [rd.choice(sample_groups[sid]) for sid in selected_ids]

            fig, axes = plt.subplots(2, 2, figsize=(6, 6), facecolor="white")

            for idx, ax_idx in enumerate(sample_indices):
                row, col = idx // 2, idx % 2
                img_path = image_metadata[ax_idx]['filename']
                img = Image.open(img_path)
                axes[row, col].imshow(img)
                # Mostra direções de preço no título
                next1 = image_metadata[ax_idx].get('price_direction_next1', 'N/A')
                next5 = image_metadata[ax_idx].get('price_direction_next5', 'N/A')
                next30 = image_metadata[ax_idx].get('price_direction_next30', 'N/A')
                axes[row, col].set_title(f"Next1: {next1}, Next5: {next5}, Next30: {next30}", fontsize=8)
                axes[row, col].axis('off')

            plt.tight_layout()
            plt.show()
        else:
            print("Aviso: Não há amostras únicas suficientes para exibir exemplos.")

    # Limpa metadados antes de retornar
    metadata_df = pd.DataFrame(image_metadata)
    metadata_df = _clean_metadata(metadata_df)

    # Aplica seleção de features CDL usando SelectKBest
    metadata_df = select_top_cdl_features(metadata_df, k=top_k_cdl_features)

    # Exporta para Google Drive se solicitado
    if export_to_drive:
        _export_to_drive(metadata_df, export_filename)

    return metadata_df

def prepare_dataset(df, image_size=(128, 128), test_split=0.2, batch_size=32):
    """
    Prepara datasets de treino e teste para modelo dual-output.

    Cria geradores customizados que retornam:
    - X: imagens normalizadas
    - y: {'cdl_patterns': array binário de padrões CDL,
          'price_directions': array binário de direções de preço}

    Args:
        df (pd.DataFrame): DataFrame com colunas 'filename', padrões CDL, e direções de preço.
        image_size (tuple): Tamanho de redimensionamento (altura, largura). Padrão: (128, 128).
        test_split (float): Proporção do teste (0.0 a 1.0). Padrão: 0.2.
        batch_size (int): Tamanho do lote para processamento. Padrão: 32.

    Returns:
        tuple: (train_generator, test_generator, cdl_columns, price_direction_columns)
    """
    print(f"\nPreparando datasets dual-output: divisão treino/teste = {1-test_split:.0%}/{test_split:.0%}")

    # Identifica colunas de padrões CDL (todas as colunas começando com 'CDL')
    cdl_columns = [col for col in df.columns if col.startswith('CDL')]
    print(f"Detectados {len(cdl_columns)} padrões CDL")

    # Colunas de direção de preço
    price_direction_columns = ['price_direction_next1', 'price_direction_next5', 'price_direction_next30']

    # Verifica se todas as colunas necessárias existem
    for col in price_direction_columns:
        if col not in df.columns:
            raise ValueError(f"Coluna '{col}' não encontrada no DataFrame")

    # Remove linhas com labels 'neutral' (dados incompletos)
    df_clean = df.copy()
    for col in price_direction_columns:
        df_clean = df_clean[df_clean[col] != 'neutral']

    if len(df_clean) < len(df):
        print(f"Removidos {len(df) - len(df_clean)} amostras com labels 'neutral'")

    # Cria chave de estratificação baseada na direção next1
    stratify_key = df_clean['price_direction_next1']

    # Divisão estratificada
    train_df, test_df = train_test_split(
        df_clean,
        test_size=test_split,
        stratify=stratify_key,
        random_state=42
    )

    print(f"Divisão: {len(train_df)} treino, {len(test_df)} teste")

    # Cria geradores customizados
    train_generator = DualOutputSequence(
        train_df,
        cdl_columns,
        price_direction_columns,
        image_size=image_size,
        batch_size=batch_size,
        shuffle=True
    )

    test_generator = DualOutputSequence(
        test_df,
        cdl_columns,
        price_direction_columns,
        image_size=image_size,
        batch_size=batch_size,
        shuffle=False
    )

    print(f"\nDatasets preparados:")
    print(f"  Treino: {len(train_df)} imagens")
    print(f"  Teste: {len(test_df)} imagens")
    print(f"  Padrões CDL: {len(cdl_columns)} features")
    print(f"  Direções de Preço: 6 saídas (3 horizontes × 2 direções)")

    return train_generator, test_generator, cdl_columns, price_direction_columns

class DualOutputSequence(tf.keras.utils.Sequence):
    """
    Gerador de Sequência customizado para modelo dual-output.

    Retorna lotes de (imagens, {'cdl_patterns': cdl_labels, 'price_directions': price_labels})
    """

    def __init__(self, dataframe, cdl_columns, price_direction_columns,
                 image_size=(128, 128), batch_size=32, shuffle=True, **kwargs):
        super().__init__(**kwargs)  # Inicializa corretamente a classe pai
        self.df = dataframe.reset_index(drop=True)
        self.cdl_columns = cdl_columns
        self.price_direction_columns = price_direction_columns
        self.image_size = image_size
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indexes = np.arange(len(self.df))
        self.on_epoch_end()

    def __len__(self):
        """Número de lotes por época"""
        return int(np.ceil(len(self.df) / self.batch_size))

    def __getitem__(self, index):
        """Gera um lote de dados"""
        # Obtém índices do lote
        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]

        # Gera dados
        X, y = self._generate_batch(batch_indexes)

        return X, y

    def on_epoch_end(self):
        """Embaralha índices após cada época"""
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def _generate_batch(self, batch_indexes):
        """Gera dados do lote"""
        batch_size = len(batch_indexes)

        # Inicializa arrays
        X = np.zeros((batch_size, *self.image_size, 3), dtype=np.float32)
        y_cdl = np.zeros((batch_size, len(self.cdl_columns)), dtype=np.float32)
        y_price = np.zeros((batch_size, 6), dtype=np.float32)

        # Gera dados para cada amostra
        for i, idx in enumerate(batch_indexes):
            row = self.df.iloc[idx]

            # Carrega e pré-processa imagem
            img_path = row['filename']
            img = Image.open(img_path)

            # Converte para RGB se imagem tiver canal alpha (RGBA) ou for escala de cinza
            if img.mode != 'RGB':
                img = img.convert('RGB')

            img = img.resize(self.image_size)
            img_array = np.array(img, dtype=np.float32) / 255.0  # Normaliza para [0, 1]
            X[i] = img_array

            # Extrai labels de padrões CDL (já binários 0/1)
            y_cdl[i] = row[self.cdl_columns].values.astype(np.float32)

            # Converte strings de direção de preço para formato binário
            # Para cada horizonte (next1, next5, next30):
            #   - 'up' → [1, 0]
            #   - 'down' → [0, 1]
            for j, col in enumerate(self.price_direction_columns):
                direction = row[col]
                if direction == 'up':
                    y_price[i, j*2] = 1.0      # up
                    y_price[i, j*2 + 1] = 0.0  # not down
                elif direction == 'down':
                    y_price[i, j*2] = 0.0      # not up
                    y_price[i, j*2 + 1] = 1.0  # down

        return X, {'cdl_patterns': y_cdl, 'price_directions': y_price}


# ============================================================================
# Funções de processamento principal
# ============================================================================

def process_ticker_samples(ticker_symbol, stock_data, num_samples, batch_size, chart_styles,
                           output_path, columns_to_normalize, column_rename_mapping):
    """
    Processa todas as amostras para um único ticker.

    Args:
        ticker_symbol: Símbolo do ticker a processar
        stock_data: DataFrame completo com todos os dados
        num_samples: Número de amostras a gerar
        batch_size: Tamanho do lote de processamento
        chart_styles: Estilos de gráficos
        output_path: Path para saída
        columns_to_normalize: Colunas a normalizar
        column_rename_mapping: Mapeamento de renomeação

    Returns:
        Lista de metadados de imagens geradas
    """
    # Tamanho fixo da amostra
    SAMPLE_SIZE = 30
    FUTURE_PERIODS_NEEDED = 30  # Necessário 30 períodos após a amostra para price_direction_next30

    print(f"\nProcessando {ticker_symbol}...")

    # Prepara e limpa dados do ticker
    ticker_data = _prepare_ticker_data(stock_data, ticker_symbol)
    if ticker_data is None:
        print(f"  {ticker_symbol}: Sem dados após limpeza - pulando ticker")
        return []

    # Calcula índice máximo válido
    # Necessário: sample_size períodos para a amostra + 30 períodos para cálculos de preço futuro
    max_starting_index = len(ticker_data) - SAMPLE_SIZE - FUTURE_PERIODS_NEEDED
    if max_starting_index <= 0:
        print(f"  {ticker_symbol}: Dados insuficientes ({len(ticker_data)} registros, necessário mínimo {SAMPLE_SIZE + FUTURE_PERIODS_NEEDED}) - pulando ticker")
        return []

    # Cria memmap para dados do ticker (economiza RAM)
    temp_memmap_path = output_path / f"temp_{ticker_symbol}.dat"
    ticker_data_memmap = _create_ticker_memmap(ticker_data, temp_memmap_path)
    ohlc_memmap = ticker_data_memmap

    # Gera índices de amostra
    first_pass_indices, unique_samples_count, available_indices = _generate_sample_indices(
        max_starting_index, num_samples
    )

    # Primeira passagem: gera amostras únicas em lotes
    used_indices = []
    images_generated = 0
    ticker_metadata = []

    # Divide índices em lotes
    batches = [first_pass_indices[i:i + batch_size] for i in range(0, len(first_pass_indices), batch_size)]

    for batch in batches:
        batch_metadata, batch_count = process_sample_batch(
            batch, ticker_data_memmap, ticker_symbol, images_generated,
            chart_styles, output_path, columns_to_normalize, column_rename_mapping,
            SAMPLE_SIZE, ohlc_memmap
        )
        ticker_metadata.extend(batch_metadata)
        images_generated += batch_count
        used_indices.extend(batch)
        gc.collect()

    # Segunda passagem: repete amostras se necessário
    if images_generated < num_samples and len(used_indices) > 0:
        repeat_needed = num_samples - images_generated
        print(f"  Gerando {repeat_needed} amostras repetidas para atingir meta de {num_samples}")

        repeat_indices = [used_indices[rd.randint(0, len(used_indices) - 1)]
                         for _ in range(repeat_needed)]
        repeat_batches = [repeat_indices[i:i + batch_size] for i in range(0, len(repeat_indices), batch_size)]

        for batch in repeat_batches:
            batch_metadata, batch_count = process_sample_batch(
                batch, ticker_data_memmap, ticker_symbol, images_generated,
                chart_styles, output_path, columns_to_normalize, column_rename_mapping,
                SAMPLE_SIZE, ohlc_memmap
            )
            ticker_metadata.extend(batch_metadata)
            images_generated += batch_count
            gc.collect()

    print(f"  {ticker_symbol}: {images_generated} amostras geradas (5 variações cada = {images_generated * 5} imagens)")

    # Limpa memmap e arquivo temporário
    del ticker_data_memmap, ohlc_memmap
    temp_memmap_path.unlink(missing_ok=True)
    del ticker_data
    gc.collect()

    return ticker_metadata

def select_top_cdl_features(df, k=20):
    """
    Seleciona os top K padrões CDL mais preditivos usando SelectKBest.

    Utiliza chi2 score para avaliar quais padrões CDL melhor explicam
    as direções de preço (next1, next5, next30). A seleção é baseada em
    um target que combina todas as três direções de preço, garantindo
    que as features selecionadas sejam úteis para todos os horizontes.

    Args:
        df (pd.DataFrame): DataFrame com colunas CDL e direções de preço
        k (int): Número de features CDL a manter (padrão: 20)

    Returns:
        pd.DataFrame: DataFrame filtrado com apenas as top K features CDL
    """
    # Identifica colunas CDL
    cdl_columns = [col for col in df.columns if col.startswith('CDL')]

    if len(cdl_columns) == 0:
        print("Aviso: Nenhum padrão CDL encontrado no DataFrame.")
        return df

    # Valida número de amostras
    if len(df) < k:
        print(f"Aviso: Amostras insuficientes ({len(df)}) para seleção de {k} features. Usando todas as features disponíveis.")
        return df

    # Ajusta k se for maior que o número de features disponíveis
    original_k = k
    k = min(k, len(cdl_columns))

    if k < original_k:
        print(f"Aviso: Apenas {len(cdl_columns)} features CDL disponíveis. Ajustando k de {original_k} para {k}.")

    print(f"\n{'='*70}")
    print(f"SELEÇÃO DE FEATURES CDL (SelectKBest com chi2)")
    print(f"{'='*70}")
    print(f"Features CDL totais: {len(cdl_columns)}")
    print(f"Features a selecionar: {k}")
    print(f"Amostras no dataset: {len(df)}")

    # Prepara X (features CDL) e y (direções de preço)
    X = df[cdl_columns].values

    # Cria um target multiclasse combinando as três direções
    # Codifica cada combinação de direções como uma classe única
    y_combined = []
    for _, row in df.iterrows():
        # Codifica: next1_next5_next30 (ex: 'up_down_up' -> 0, 'down_up_down' -> 1, etc)
        direction_combo = f"{row['price_direction_next1']}_{row['price_direction_next5']}_{row['price_direction_next30']}"
        y_combined.append(direction_combo)

    # Converte para labels numéricas
    le = LabelEncoder()
    y_encoded = le.fit_transform(y_combined)

    print(f"Classes de direção únicas: {len(le.classes_)}")

    # Aplica SelectKBest com chi2
    selector = SelectKBest(score_func=chi2, k=k)
    selector.fit(X, y_encoded)

    # Obtém as features selecionadas e seus scores
    selected_mask = selector.get_support()
    feature_scores = selector.scores_

    # Cria lista de (feature, score) para as selecionadas
    selected_features_with_scores = [
        (col, score)
        for col, selected, score in zip(cdl_columns, selected_mask, feature_scores)
        if selected
    ]
    # Ordena por score (maior primeiro)
    selected_features_with_scores.sort(key=lambda x: x[1], reverse=True)

    selected_cdl_columns = [col for col, _ in selected_features_with_scores]

    print(f"\nTop {k} Features CDL Selecionadas (ordenadas por score):")
    print(f"{'-'*70}")
    for i, (feature, score) in enumerate(selected_features_with_scores, 1):
        print(f"  {i:2d}. {feature:25s} (score: {score:8.2f})")
    print(f"{'='*70}\n")

    # Filtra o DataFrame para manter apenas as colunas selecionadas + outras colunas
    non_cdl_columns = [col for col in df.columns if not col.startswith('CDL')]
    final_columns = non_cdl_columns + selected_cdl_columns

    return df[final_columns]

def process_sample_batch(batch_indices, ticker_data_memmap, ticker_symbol, images_generated_start,
                        chart_styles, output_path, columns_to_normalize, column_rename_mapping,
                        sample_size, ohlc_memmap):
    """
    Processa um lote de amostras.

    Args:
        batch_indices: Lista de índices para processar no lote
        ticker_data_memmap: Memmap com dados do ticker
        ticker_symbol: Símbolo do ticker
        images_generated_start: Número inicial de imagens geradas
        chart_styles: Lista de estilos de gráfico
        output_path: Path para diretório de saída
        columns_to_normalize: Colunas para normalizar
        column_rename_mapping: Mapeamento de renomeação de colunas
        sample_size: Tamanho da amostra (fixo em 30)
        ohlc_memmap: Memmap OHLC para cálculo de padrões

    Returns:
        Tupla (batch_metadata, images_count) com metadados e contagem de imagens
    """
    batch_metadata = []
    images_generated = images_generated_start

    for starting_index in batch_indices:
        # Extrai amostra do memmap
        sample_data = ticker_data_memmap[starting_index:starting_index + sample_size]

        if not sample_data.shape[0] == sample_size:
            print(f"  Aviso: Dados da amostra com tamanho incorreto ({sample_data.shape[0]} em vez de {sample_size}) - pulando amostra no índice {starting_index}")
            continue

        sample_candles = pd.DataFrame(sample_data, columns=ticker_data_memmap.dtype.names)

        # Cria DatetimeIndex para mplfinance
        sample_candles.index = pd.date_range(start='2020-01-01', periods=len(sample_candles), freq='D')

        if len(sample_candles) != sample_size or sample_candles.isnull().any().any():
            print(f"  Aviso: Dados da amostra nulos ou com tamanho incorreto ({len(sample_candles)}) - pulando amostra no índice {starting_index}")
            continue

        # Calcula todos os padrões e direções de preço
        pattern_data = calculate_all_patterns_and_directions(ohlc_memmap, starting_index, sample_size)
        if pattern_data is None:
            continue  # Pula amostras onde padrões não podem ser calculados

        # Normaliza amostra
        sample_candles = normalize_sample(sample_candles, columns_to_normalize, column_rename_mapping)

        # Gera variações
        try:
            generate_image_variations(sample_candles, ticker_symbol, images_generated, chart_styles,
                                    output_path, pattern_data, batch_metadata)

            del sample_candles
            images_generated += 1

        except Exception as error:
            print(f"  Erro ao gerar imagem {images_generated + 1}: {error}")
            continue

    gc.collect()
    return batch_metadata, images_generated - images_generated_start


# ============================================================================
# Auxiliares de geração de amostra
# ============================================================================

def calculate_all_patterns_and_directions(ohlc_values, starting_index, sample_size=30):
    """
    Calcula TODOS os padrões de candlestick do TA-Lib e direções de preço futuras.

    Para cada amostra, detecta todos os padrões disponíveis no TA-Lib e calcula
    a direção do preço para os próximos 1, 5 e 30 períodos.

    Args:
        ohlc_values: Array numpy memmap com dados OHLC
        starting_index: Índice inicial da amostra
        sample_size: Tamanho da amostra em candles (fixo em 30)

    Returns:
        Dictionary com:
        - Uma chave por padrão TA-Lib (ex: 'CDL3BLACKCROWS': 1 ou 0)
        - 'price_direction_next1': 'up', 'down', ou 'neutral'
        - 'price_direction_next5': 'up', 'down', ou 'neutral'
        - 'price_direction_next30': 'up', 'down', ou 'neutral'

        Retorna None se não for possível calcular
    """
    end_idx = starting_index + sample_size
    sample_data = ohlc_values[starting_index:end_idx]

    # Valida tamanho da amostra
    if sample_data.shape[0] != sample_size:
        return None

    sample_df = pd.DataFrame(sample_data, columns=ohlc_values.dtype.names)

    # Extrai OHLC
    opens = sample_df['open'].values
    highs = sample_df['high'].values
    lows = sample_df['low'].values
    closes = sample_df['close'].values

    # Verifica se arrays OHLC são válidos
    if not opens.size > 0:
        return None

    # Inicializa dicionário de resultados
    result = {}

    # Usa funções de reconhecimento de padrão TA-Lib em cache para desempenho
    pattern_functions = _TALIB_PATTERN_FUNCTIONS

    # Calcula todos os padrões de candlestick
    for pattern_name in pattern_functions:
        try:
            pattern_result = getattr(talib, pattern_name)(opens, highs, lows, closes)
            # Verifica se ALGUM valor diferente de zero existe em toda a amostra
            # Armazena 1 se padrão detectado em qualquer lugar, 0 caso contrário
            result[pattern_name] = 1 if np.any(pattern_result != 0) else 0
        except Exception as e:
            # Se padrão falhar, marca como não detectado
            result[pattern_name] = 0

    # Calcula direções de preço para next1, next5, next30
    last_close = closes[-1]

    # Próximo 1 período
    if end_idx < len(ohlc_values):
        next1_close = ohlc_values[end_idx]['close']
        result['price_direction_next1'] = 'up' if next1_close > last_close else 'down'
    else:
        result['price_direction_next1'] = 'neutral'

    # Próximos 5 períodos
    if end_idx + 4 < len(ohlc_values):
        next5_close = ohlc_values[end_idx + 4]['close']
        result['price_direction_next5'] = 'up' if next5_close > last_close else 'down'
    else:
        result['price_direction_next5'] = 'neutral'

    # Próximos 30 períodos
    if end_idx + 29 < len(ohlc_values):
        next30_close = ohlc_values[end_idx + 29]['close']
        result['price_direction_next30'] = 'up' if next30_close > last_close else 'down'
    else:
        result['price_direction_next30'] = 'neutral'

    return result

def normalize_sample(sample_candles, columns_to_normalize, column_rename_mapping):
    """
    Normaliza as colunas e renomeia para formato esperado pelo mplfinance.

    Args:
        sample_candles: DataFrame com dados do candle
        columns_to_normalize: Lista de colunas para normalizar
        column_rename_mapping: Dicionário de mapeamento para renomear colunas

    Returns:
        DataFrame normalizado e renomeado
    """
    sample_normalized = sample_candles.copy()
    for column_name in columns_to_normalize:
        column_min = sample_normalized[column_name].min()
        column_max = sample_normalized[column_name].max()
        if column_max != column_min:
            sample_normalized[column_name] = (sample_normalized[column_name] - column_min) / (column_max - column_min)
        else:
            sample_normalized[column_name] = 0
    sample_normalized.rename(columns=column_rename_mapping, inplace=True)
    return sample_normalized

def generate_image_variations(sample_candles, ticker_symbol, images_generated, chart_styles,
                              output_path, pattern_data, image_metadata):
    """
    Gera cinco variações de imagem com estilos diferentes (sem repetição).

    Args:
        sample_candles: DataFrame normalizado com dados do candle
        ticker_symbol: Símbolo do ticker
        images_generated: Número de imagens já geradas
        chart_styles: Lista de estilos de gráfico disponíveis
        output_path: Path para diretório de saída
        pattern_data: Dictionary com detecções de padrões e direções de preço
        image_metadata: Lista para acumular metadados (modificada in-place)
    """
    # Seleciona 5 estilos únicos dos estilos disponíveis (sem repetição)
    style_indices = rd.sample(range(len(chart_styles)), 5)

    for variation in range(1, 6):
        image_filename = f"{ticker_symbol}_sample_{images_generated + 1}_{variation}.png"
        image_full_path = output_path / image_filename

        selected_style = chart_styles[style_indices[variation - 1]]

        volume = rd.choice([True, False])

        plot_kwargs = dict(
            data=sample_candles,
            type='candle',
            style=selected_style,
            volume=volume,
            figratio=(1, 1),
            figsize=(3, 3),
            returnfig=True
        )

        chart_figure, chart_axes = mpf.plot(**plot_kwargs)

        # Remove eixos para focar apenas nos padrões visuais
        for axis in chart_axes:
            ymin, ymax = axis.get_ylim()
            if ymin == ymax:
                axis.set_ylim(ymin - 0.5, ymax + 0.5)
            axis.set_yticks([])
            axis.set_xticks([])
            axis.set_ylabel("")

        chart_figure.savefig(image_full_path, dpi=100, bbox_inches='tight')
        plt.close(chart_figure)

        del chart_figure, chart_axes

        # Cria entrada de metadados com nome do arquivo e todos os dados de padrão
        metadata_entry = {'filename': str(image_full_path.resolve())}
        metadata_entry.update(pattern_data)  # Adiciona todas as detecções de padrão e direções de preço
        image_metadata.append(metadata_entry)

def _prepare_ticker_data(stock_data, ticker_symbol):
    """
    Prepara dados do ticker removendo valores inválidos.

    Returns:
        DataFrame limpo ou None se não houver dados válidos
    """
    ticker_data = stock_data[stock_data['ticker'] == ticker_symbol].copy()

    # Remove valores inválidos
    ticker_data.replace([np.inf, -np.inf], np.nan, inplace=True)
    ticker_data.dropna(inplace=True)

    if ticker_data.empty:
        return None

    return ticker_data

def _create_ticker_memmap(ticker_data, temp_path):
    """Cria memmap para dados do ticker."""
    ticker_data_array = ticker_data[['open', 'high', 'low', 'close', 'volume']].to_records(index=False)
    ticker_data_memmap = np.memmap(temp_path, dtype=ticker_data_array.dtype,
                                   mode='w+', shape=ticker_data_array.shape)
    ticker_data_memmap[:] = ticker_data_array[:]
    ticker_data_memmap.flush()
    return ticker_data_memmap

def _generate_sample_indices(max_starting_index, num_samples):
    """
    Gera índices embaralhados para amostras únicas e repetidas.

    Returns:
        Tupla (first_pass_indices, unique_samples_count, all_available_indices)
    """
    # Usa rd.sample para embaralhamento eficiente sem criar lista intermediária
    available_indices = rd.sample(range(max_starting_index), max_starting_index)

    max_unique_samples = len(available_indices)
    unique_samples_count = min(num_samples, max_unique_samples)

    first_pass_indices = available_indices[:unique_samples_count]

    return first_pass_indices, unique_samples_count, available_indices


# ============================================================================
# Funções utilitárias
# ============================================================================

def _clean_metadata(metadata_df):
    """
    Limpa DataFrame de metadados removendo valores inválidos.

    Remove NaN, infinitos e strings vazias/apenas com espaços.

    Args:
        metadata_df: DataFrame com metadados de imagens

    Returns:
        DataFrame limpo
    """
    if metadata_df.empty:
        return metadata_df

    rows_before = len(metadata_df)

    # Remove NaN, infinitos e strings vazias (encadeado para eficiência)
    metadata_df = metadata_df.replace([np.inf, -np.inf], np.nan).dropna()

    # Remove strings vazias ou apenas com espaços em colunas de objeto
    for column_name in metadata_df.columns:
        if metadata_df[column_name].dtype == 'object':
            metadata_df = metadata_df[metadata_df[column_name].str.strip().astype(bool)]

    rows_after = len(metadata_df)
    rows_removed = rows_before - rows_after

    if rows_removed > 0:
        print(f"\nLimpeza de metadados: {rows_removed} entradas inválidas removidas")
        print(f"Total após limpeza: {rows_after} imagens")

    return metadata_df

def _export_to_drive(metadata_df, file_name='dataset'):
    """
    Exporta metadados e imagens para Google Drive em formato zip.

    Args:
        metadata_df: DataFrame com metadados de imagens
        file_name: Nome do arquivo zip (sem extensão)

    Returns:
        bool: True se exportado com sucesso, False caso contrário
    """
    print("\nPreparando exportação para Google Drive...")

    drive_path = Path('/content/drive')
    if not drive_path.exists() or not drive_path.is_mount():
        print("Erro: Google Drive não montado. Execute: from google.colab import drive; drive.mount('/content/drive')")
        return False

    temp_dir = Path('/content/temp_dataset')
    temp_dir.mkdir(parents=True, exist_ok=True)

    # Salva metadados como CSV
    metadata_csv_path = temp_dir / 'metadata.csv'
    metadata_df.to_csv(metadata_csv_path, index=False)

    # Cria arquivo zip com CSV e imagens
    zip_filename = f"{file_name}.zip"
    zip_path = temp_dir / zip_filename

    missing_files = 0
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        zipf.write(metadata_csv_path, arcname='metadata.csv')

        # Adiciona imagens preservando estrutura de pastas
        for _, row in metadata_df.iterrows():
            filename_with_path = row['filename']
            source_file = Path(filename_with_path)

            if source_file.exists():
                zipf.write(source_file, arcname=str(source_file))
            else:
                missing_files += 1

    if missing_files > 0:
        print(f"Aviso: {missing_files} arquivos não encontrados")

    zip_size_mb = zip_path.stat().st_size / (1024 * 1024)
    print(f"Zip criado: {zip_size_mb:.2f} MB")

    # Move para Google Drive
    drive_destination = Path('/content/drive/MyDrive') / zip_filename
    shutil.move(str(zip_path), str(drive_destination))

    print(f"Dataset salvo: {drive_destination}")

    shutil.rmtree(temp_dir)
    return True


# ============================================================================
# Auxiliares de configuração
# ============================================================================

def _get_chart_styles():
    """Retorna lista de estilos de gráfico disponíveis."""
    return ['binance', 'default', 'charles', 'classic', 'checkers', 'sas', 'yahoo']

def _get_normalization_config():
    """Retorna configuração de normalização."""
    columns_to_normalize = ['open', 'high', 'low', 'close', 'volume']
    column_rename_mapping = {'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'}
    return columns_to_normalize, column_rename_mapping

"""# Gerando datasets
Utilizando as funções criadas acima, nessa seção são gerados datasets que podem ser utilizados para treinamento, validação e teste do modelo.
"""

## Gerando dataset inicial com 50 ações, 80 amostras por ação e 5 gráficos por amostra = 20.000 gráficos
## Utilizando intervalos de 1min, em um período de 5d
tickers = get_random_tickers(num_tickers=75)
num_samples = 100
period = '5d'
interval = '1m'
directory='/content/samples'

stocks = download_data(tickers, period=period, interval=interval)
samples = generate_samples(stocks, num_samples=num_samples, output_directory=directory)

## Gerando datasets de treinamento e teste utilizando geradores customizados para otimizar memória
# O dataset é gerado dinamicamente a partir do DataFrame processado
# Retorna X (imagens) e y (dicionário com padrões CDL e direções de preço)
train_generator, test_generator, cdl_columns, price_direction_columns = prepare_dataset(
    samples,
    image_size=(128, 128),
    test_split=0.2,
    batch_size=32
)

"""# Arquitetura e treinamento da rede
Nesta seção, é definida a arquitetura da rede neural, bem como é realizado o seu treinamento com base nos datasets gerados na seção anterior. A arquitetura dessa rede é inspirada na arquitetura da LeNet-5, mas com algumas otimizações mais modernas, a exemplo do uso da ReLU como função de ativação das camadas convolucionais e da adição de mais um bloco convolucional para aumentar a profundidade da imagem. O dropout é definido como 30% em cada bloco, o filtro tem 3x3px em todas as camadas, e o stride e o padding não foram modificados.
"""

## Definição da arquitetura da rede utilizada
# Arquitetura CNN + MLP de saída dupla (Dual-output):
# - CNN extrai características visuais dos gráficos de candlestick
# - Primeira saída: prevê todos os padrões CDL do TA-Lib (camada Dense intermediária)
# - Segunda saída: MLP prevê 6 direções de preço binárias (next1/5/30 up/down)
def do_network(cdl_columns, dropout_rate=0.35):
    """
    Cria um modelo de saída dupla para análise de padrões de candlestick.

    Args:
        cdl_columns: Lista de colunas CDL do dataset (retornado por prepare_dataset).
                    O número de padrões é automaticamente inferido do tamanho da lista.
        dropout_rate: Taxa de dropout aplicada a todas as camadas (padrão: 0.3).
                     Valores típicos: 0.2 (menos regularização) a 0.5 (mais regularização).

    Returns:
        Modelo Keras com duas saídas: 'cdl_patterns' e 'price_directions'
    """
    # Infere automaticamente o número de padrões CDL da lista de colunas
    num_cdl_patterns = len(cdl_columns)
    print(f"\nCriando modelo com {num_cdl_patterns} padrões CDL (após SelectKBest)")

    inputs = tf.keras.layers.Input(shape=(128,128,3), name='input_image')

    # ========================================================================
    # Extrator de Características CNN (Inspirado na LeNet-5 com otimizações modernas)
    # ========================================================================

    # Bloco 1: 32 filtros
    c1 = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)
    c1 = tf.keras.layers.BatchNormalization()(c1)
    c1 = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(c1)
    s2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(c1)
    s2 = tf.keras.layers.Dropout(dropout_rate)(s2)

    # Bloco 2: 64 filtros
    c3 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(s2)
    c3 = tf.keras.layers.BatchNormalization()(c3)
    c3 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')(c3)
    s4 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(c3)
    s4 = tf.keras.layers.Dropout(dropout_rate)(s4)

    # Bloco 3: 128 filtros
    c5 = tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu')(s4)
    c5 = tf.keras.layers.BatchNormalization()(c5)
    c5 = tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu')(c5)
    s6 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(c5)
    s6 = tf.keras.layers.Dropout(dropout_rate)(s6)

    # Achata características da CNN
    flat = tf.keras.layers.Flatten()(s6)

    # Camadas densas para processamento de características
    f7 = tf.keras.layers.Dense(256, activation='relu')(flat)
    f7 = tf.keras.layers.BatchNormalization()(f7)
    f7 = tf.keras.layers.Dropout(dropout_rate)(f7)
    f8 = tf.keras.layers.Dense(128, activation='relu')(f7)
    f8 = tf.keras.layers.BatchNormalization()(f8)
    f8 = tf.keras.layers.Dropout(dropout_rate)(f8)

    # ========================================================================
    # Saída 1: Predições de Padrões CDL (classificação multi-label)
    # Prevê quais padrões de candlestick estão presentes na imagem
    # ========================================================================
    cdl_patterns = tf.keras.layers.Dense(
        num_cdl_patterns,
        activation='sigmoid',  # sigmoid para classificação binária multi-label
        name='cdl_patterns'
    )(f8)

    # ========================================================================
    # Cabeça MLP: Predições de Direção de Preço
    # Recebe as predições de padrões CDL como entrada
    # ========================================================================
    mlp_hidden = tf.keras.layers.Dense(128, activation='relu')(cdl_patterns)
    mlp_hidden = tf.keras.layers.BatchNormalization()(mlp_hidden)
    mlp_hidden = tf.keras.layers.Dropout(dropout_rate)(mlp_hidden)

    # Saída 2: Predições de Direção de Preço (6 saídas binárias)
    # [next1_up, next1_down, next5_up, next5_down, next30_up, next30_down]
    price_directions = tf.keras.layers.Dense(
        6,
        activation='sigmoid',  # sigmoid para classificação binária multi-label
        name='price_directions'
    )(mlp_hidden)

    return tf.keras.models.Model(
        inputs=inputs,
        outputs=[cdl_patterns, price_directions],
        name='cnn_mlp_dual_output'
    )

## Cria modelo usando colunas CDL do dataset (após SelectKBest)
# O número de padrões CDL é automaticamente inferido pela função do_network()
model = do_network(cdl_columns)
model.summary()

## Compilando e definindo checkpoints do modelo para otimizar treinamento
# Usando binary crossentropy para ambas as saídas pois são classificações binárias multi-label
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss={
        'cdl_patterns': 'binary_crossentropy',      # Predição multi-label de padrões CDL
        'price_directions': 'binary_crossentropy'   # Predição multi-label de direção de preço
    },
    metrics={
        # Usando BinaryAccuracy para classificação multi-label (avalia cada saída independentemente)
        # 'accuracy' regular é inapropriada pois espera que TODAS as labels coincidam exatamente
        'cdl_patterns': [tf.keras.metrics.BinaryAccuracy(name='binary_acc')],
        'price_directions': [tf.keras.metrics.BinaryAccuracy(name='binary_acc')]
    }
)

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath='best_model.weights.h5',
    save_weights_only=True,
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    verbose=1
)

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=25,
    restore_best_weights=True
)

## Treinando o modelo para 100 épocas (usando geradores customizados por conta de memória)
epochs = 200

H = model.fit(
    train_generator,
    validation_data=test_generator,
    epochs=epochs,
    callbacks=[model_checkpoint_callback, early_stopping]
)

## Apresentando resultados do modelo com dados de teste
results = model.evaluate(test_generator, steps=len(test_generator))

print(f"\nFinal Test Results:")
print(f"  Total Loss: {results[0]:.4f}")
print(f"  CDL Patterns Loss: {results[1]:.4f}")
print(f"  Price Directions Loss: {results[2]:.4f}")
print(f"  CDL Patterns Binary Accuracy: {results[3]:.4f}")
print(f"  Price Directions Binary Accuracy: {results[4]:.4f}")

## Reaproveitando código de aula para plotar a matriz de confusão e calcular métricas
# Prevendo os valores de Y para os Xs de teste
# O modelo retorna [cdl_predictions, price_predictions]
predictions = model.predict(test_generator)
cdl_pred, price_pred = predictions

# ============================================================================
# AVALIAÇÃO 1: DIREÇÕES DE PREÇO (PRICE DIRECTIONS)
# ============================================================================

# Definindo horizontes e seus índices correspondentes na saída
horizons = [
    {'name': 'next1', 'start_idx': 0, 'end_idx': 2},
    {'name': 'next5', 'start_idx': 2, 'end_idx': 4},
    {'name': 'next30', 'start_idx': 4, 'end_idx': 6}
]

# Obtendo labels verdadeiros do gerador de teste
all_true_price_labels = []
all_true_cdl_labels = []

for i in range(len(test_generator)):
    _, labels = test_generator[i]
    all_true_price_labels.append(labels['price_directions'])
    all_true_cdl_labels.append(labels['cdl_patterns'])

all_true_price_labels = np.concatenate(all_true_price_labels)
all_true_cdl_labels = np.concatenate(all_true_cdl_labels)

# Iterando sobre cada horizonte para avaliar
for horizon in horizons:
    name = horizon['name']
    start = horizon['start_idx']
    end = horizon['end_idx']

    print(f"\n{'='*40}")
    print(f"AVALIAÇÃO PARA: Direção de Preço {name.upper()}")
    print(f"{'='*40}")

    # Extraindo predições para este horizonte
    horizon_pred = price_pred[:, start:end]
    y_pred_class = np.argmax(horizon_pred, axis=1)

    # Extraindo labels verdadeiros para este horizonte
    horizon_true = all_true_price_labels[:, start:end]
    y_true_class = np.argmax(horizon_true, axis=1)

    # Criando Matriz de Confusão
    cm = confusion_matrix(y_true_class, y_pred_class)

    # Plotando Matriz de Confusão
    fig, ax = plt.subplots(figsize=(6, 5))
    display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['up', 'down'])
    display.plot(ax=ax)
    ax.set_title(f'Matriz de Confusão - Direção {name}')
    plt.show()

    # Calculando Métricas
    # Acurácia Geral
    if np.sum(cm) > 0:
        overallAccuracy = np.sum(np.diag(cm)) / np.sum(cm)
    else:
        overallAccuracy = 0.0
    print(f"\nAcurácia Geral ({name}): {overallAccuracy:.4f}")
    print("-----")

    for i, label in enumerate(['up', 'down']):
        # Erro de Omissão
        referenceValues = np.sum(cm[i,:])
        correctlyClassified = cm[i, i]
        omittedValues = referenceValues - correctlyClassified
        omissionError = round(omittedValues / referenceValues*100, 4) if referenceValues > 0 else 0
        print(f"Erro de omissão para '{label}' = {omissionError}%")

        # Erro de Comissão
        classifiedValues = np.sum(cm[:,i])
        correctlyClassified_1 = cm[i, i]
        committedValues = classifiedValues - correctlyClassified_1
        commissionError = round(committedValues / classifiedValues*100, 4) if classifiedValues > 0 else 0
        print(f"Erro de comissão para '{label}' = {commissionError}%")

        print("-----")

    # F1-Score
    f1 = f1_score(y_true_class, y_pred_class, average='weighted')
    print(f"F1 Score (ponderado) para {name}: {f1:.4f}")


# ============================================================================
# AVALIAÇÃO 2: PADRÕES DE CANDLESTICK (CDL PATTERNS)
# ============================================================================

print(f"\n{'='*40}")
print(f"AVALIAÇÃO PARA: Padrões de Candlestick (CDL)")
print(f"{'='*40}")

# Convertendo probabilidades para classes binárias (threshold 0.5)
cdl_pred_binary = (cdl_pred > 0.5).astype(int)

# Calculando métricas gerais para multi-label
print("\nRelatório de Classificação para Padrões CDL:")
print(classification_report(all_true_cdl_labels, cdl_pred_binary,
                            target_names=cdl_columns, zero_division=0))

# Calculando Acurácia de Subset (todas as labels devem estar corretas para contar)
subset_accuracy = accuracy_score(all_true_cdl_labels, cdl_pred_binary)
print(f"Acurácia de Subset (Exact Match): {subset_accuracy:.4f}")

# Calculando Hamming Loss (fração de labels incorretas)
hamming_loss_val = hamming_loss(all_true_cdl_labels, cdl_pred_binary)
print(f"Hamming Loss: {hamming_loss_val:.4f}")

## Reaproveitando código de aula para gerar gráfico da evolução da acurácia e da perda
# Plot 1: Métricas para Direções de Preço (objetivo principal)
fig, (ax1, ax3) = plt.subplots(2, 1, figsize=(20, 12))

# --- Gráfico de Direção de Preço ---
color = 'tab:red'
ax1.set_xlabel('Época')
ax1.set_xticks(np.arange(epochs))
ax1.set_ylabel('Acurácia Direção de Preço', color=color)
ax1.plot(H.history["val_price_directions_binary_acc"], '--', label="val_acc", color=color)
ax1.plot(H.history["price_directions_binary_acc"], label="train_acc", color=color)
ax1.tick_params(axis='y', labelcolor=color)
ax1.legend(loc='upper left')
ax1.set_title('Histórico de Treinamento - Direções de Preço')

ax2 = ax1.twinx()  # instancia um segundo eixo que compartilha o mesmo eixo x

color = 'tab:blue'
ax2.set_ylabel('Perda Direção de Preço', color=color)
ax2.plot(H.history["val_price_directions_loss"], '--', label="val_loss", color=color)
ax2.plot(H.history["price_directions_loss"], label="train_loss",color=color)
ax2.tick_params(axis='y', labelcolor=color)
ax2.legend(loc='upper right')

# --- Gráfico de Padrões CDL ---
color = 'tab:green'
ax3.set_xlabel('Época')
ax3.set_xticks(np.arange(epochs))
ax3.set_ylabel('Acurácia Padrões CDL', color=color)
ax3.plot(H.history["val_cdl_patterns_binary_acc"], '--', label="val_acc", color=color)
ax3.plot(H.history["cdl_patterns_binary_acc"], label="train_acc", color=color)
ax3.tick_params(axis='y', labelcolor=color)
ax3.legend(loc='upper left')
ax3.set_title('Histórico de Treinamento - Padrões CDL')

ax4 = ax3.twinx()  # instancia um segundo eixo que compartilha o mesmo eixo x

color = 'tab:purple'
ax4.set_ylabel('Perda Padrões CDL', color=color)
ax4.plot(H.history["val_cdl_patterns_loss"], '--', label="val_loss", color=color)
ax4.plot(H.history["cdl_patterns_loss"], label="train_loss",color=color)
ax4.tick_params(axis='y', labelcolor=color)
ax4.legend(loc='upper right')

plt.tight_layout()
plt.show()